{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f958e810",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff41f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install monai nibabel torch torchvision scikit-learn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "from monai.networks.nets import SEResNet50\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce5076",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== MODIFY THESE PATHS ==============\n",
    "# Path to straightened CT volumes\n",
    "CT_FOLDER = \"/content/verse19/straighten/CT\"  # Colab path\n",
    "# CT_FOLDER = \"d:/Graduation Project/HeathiVert/verse19/straighten/CT\"  # Windows path\n",
    "\n",
    "# Path to vertebra_data.json with ground truth labels\n",
    "JSON_PATH = \"/content/verse19/vertebra_data_test.json\"  # Colab path  \n",
    "# JSON_PATH = \"d:/Graduation Project/HeathiVert/verse19/vertebra_data_test.json\"  # Windows path\n",
    "\n",
    "# Output folder for checkpoints\n",
    "CHECKPOINT_FOLDER = \"/content/checkpoints/classifier_kfold\"  # Colab path\n",
    "# CHECKPOINT_FOLDER = \"d:/Graduation Project/HeathiVert/checkpoints/classifier_kfold\"  # Windows path\n",
    "\n",
    "# ============== TRAINING HYPERPARAMETERS ==============\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NUM_SLICES = 30  # Number of slices to extract from each volume (center ± 15)\n",
    "N_FOLDS = 5  # Number of folds for cross-validation\n",
    "\n",
    "# Create checkpoint folder\n",
    "os.makedirs(CHECKPOINT_FOLDER, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_FOLDER}\")\n",
    "print(f\"Using {N_FOLDS}-fold cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a23b04",
   "metadata": {},
   "source": [
    "## 3. Load Ground Truth Labels\n",
    "\n",
    "The JSON structure is:\n",
    "```json\n",
    "{\n",
    "  \"train\": {\"sub-verse004_ct_23\": 0, \"sub-verse020_12\": 1, ...},\n",
    "  \"test\": {...},\n",
    "  \"val\": {...}\n",
    "}\n",
    "```\n",
    "\n",
    "We convert Genant grades (0,1,2,3) to binary (0=healthy, 1=fractured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10404cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(json_path):\n",
    "    \"\"\"Load vertebra labels from JSON and convert to binary classification.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Combine all splits\n",
    "    all_labels = {}\n",
    "    for split in ['train', 'test', 'val']:\n",
    "        if split in data:\n",
    "            all_labels.update(data[split])\n",
    "    \n",
    "    # Convert to binary: 0 = healthy, 1+ = fractured\n",
    "    binary_labels = {k: (1 if v > 0 else 0) for k, v in all_labels.items()}\n",
    "    \n",
    "    # Statistics\n",
    "    n_healthy = sum(1 for v in binary_labels.values() if v == 0)\n",
    "    n_fractured = sum(1 for v in binary_labels.values() if v == 1)\n",
    "    \n",
    "    print(f\"Total vertebrae: {len(binary_labels)}\")\n",
    "    print(f\"  Healthy (0): {n_healthy} ({100*n_healthy/len(binary_labels):.1f}%)\")\n",
    "    print(f\"  Fractured (1): {n_fractured} ({100*n_fractured/len(binary_labels):.1f}%)\")\n",
    "    \n",
    "    return binary_labels\n",
    "\n",
    "labels = load_labels(JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9255ec1",
   "metadata": {},
   "source": [
    "## 4. Dataset Class\n",
    "\n",
    "**Key Points:**\n",
    "- Each `.nii.gz` file is a single straightened vertebra (not whole spine)\n",
    "- File naming: `sub-verse004_ct_23.nii.gz` → lookup key: `sub-verse004_ct_23`\n",
    "- Extract **middle 30 slices** (z_center ± 15) as 2D images\n",
    "- Each slice gets the same label as its parent vertebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertebraSliceDataset(Dataset):\n",
    "    \"\"\"Dataset that extracts 2D slices from 3D straightened vertebra volumes.\"\"\"\n",
    "    \n",
    "    def __init__(self, ct_folder, labels_dict, num_slices=30, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ct_folder: Path to folder containing .nii.gz files\n",
    "            labels_dict: Dictionary mapping vertebra_id -> binary label\n",
    "            num_slices: Number of slices to extract from each volume (center ± num_slices/2)\n",
    "            transform: Optional transforms to apply to each slice\n",
    "        \"\"\"\n",
    "        self.ct_folder = Path(ct_folder)\n",
    "        self.labels_dict = labels_dict\n",
    "        self.num_slices = num_slices\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Find all .nii.gz files and their labels\n",
    "        self.samples = []  # List of (nii_path, slice_idx, label)\n",
    "        self._prepare_samples()\n",
    "        \n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Build list of (file_path, slice_index, label) tuples.\"\"\"\n",
    "        nii_files = list(self.ct_folder.glob('*.nii.gz'))\n",
    "        print(f\"Found {len(nii_files)} .nii.gz files\")\n",
    "        \n",
    "        matched = 0\n",
    "        unmatched = []\n",
    "        \n",
    "        for nii_path in nii_files:\n",
    "            # Extract vertebra ID from filename\n",
    "            # e.g., \"sub-verse004_ct_23.nii.gz\" -> \"sub-verse004_ct_23\"\n",
    "            vertebra_id = nii_path.stem.replace('.nii', '')\n",
    "            \n",
    "            # Lookup label\n",
    "            if vertebra_id in self.labels_dict:\n",
    "                label = self.labels_dict[vertebra_id]\n",
    "                matched += 1\n",
    "                \n",
    "                # Load volume to get dimensions\n",
    "                try:\n",
    "                    vol = nib.load(str(nii_path))\n",
    "                    z_dim = vol.shape[2]\n",
    "                    z_center = z_dim // 2\n",
    "                    half_slices = self.num_slices // 2\n",
    "                    \n",
    "                    # Extract middle slices\n",
    "                    start = max(0, z_center - half_slices)\n",
    "                    end = min(z_dim, z_center + half_slices)\n",
    "                    \n",
    "                    for slice_idx in range(start, end):\n",
    "                        self.samples.append((nii_path, slice_idx, label))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {nii_path}: {e}\")\n",
    "            else:\n",
    "                unmatched.append(vertebra_id)\n",
    "        \n",
    "        print(f\"Matched: {matched} vertebrae\")\n",
    "        print(f\"Unmatched: {len(unmatched)} vertebrae\")\n",
    "        print(f\"Total slices: {len(self.samples)}\")\n",
    "        \n",
    "        # Class distribution\n",
    "        n_healthy = sum(1 for _, _, l in self.samples if l == 0)\n",
    "        n_fractured = sum(1 for _, _, l in self.samples if l == 1)\n",
    "        print(f\"Slice distribution: Healthy={n_healthy}, Fractured={n_fractured}\")\n",
    "        \n",
    "        if unmatched[:5]:\n",
    "            print(f\"Sample unmatched IDs: {unmatched[:5]}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        nii_path, slice_idx, label = self.samples[idx]\n",
    "        \n",
    "        # Load volume and extract slice\n",
    "        vol = nib.load(str(nii_path)).get_fdata()\n",
    "        slice_2d = vol[:, :, slice_idx].astype(np.float32)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        slice_min, slice_max = slice_2d.min(), slice_2d.max()\n",
    "        if slice_max > slice_min:\n",
    "            slice_2d = (slice_2d - slice_min) / (slice_max - slice_min)\n",
    "        \n",
    "        # Add channel dimension: (H, W) -> (1, H, W)\n",
    "        slice_2d = np.expand_dims(slice_2d, axis=0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        slice_tensor = torch.from_numpy(slice_2d)\n",
    "        \n",
    "        if self.transform:\n",
    "            slice_tensor = self.transform(slice_tensor)\n",
    "        \n",
    "        return slice_tensor, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e4d48",
   "metadata": {},
   "source": [
    "## 5. Setup K-Fold Cross-Validation\n",
    "\n",
    "We'll use 5-fold cross-validation to better evaluate model performance and reduce overfitting to a single train/val split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7d843",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ae7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full dataset (no split yet)\n",
    "full_dataset = VertebraSliceDataset(\n",
    "    ct_folder=CT_FOLDER,\n",
    "    labels_dict=labels,\n",
    "    num_slices=NUM_SLICES,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "print(f\"Total dataset samples: {len(full_dataset)}\")\n",
    "\n",
    "# Setup K-Fold cross-validation\n",
    "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for each fold\n",
    "fold_results = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'confusion_matrices': []\n",
    "}\n",
    "\n",
    "print(f\"\\nK-Fold Setup: {N_FOLDS} folds, {len(full_dataset)} total samples\")\n",
    "print(f\"Approx. samples per fold: Train={int(len(full_dataset)*(N_FOLDS-1)/N_FOLDS)}, Val={int(len(full_dataset)/N_FOLDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary loader for visualization\n",
    "temp_loader = DataLoader(full_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "# Get a batch\n",
    "images, labels_batch = next(iter(temp_loader))\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Labels: {labels_batch}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(images):\n",
    "        ax.imshow(images[i, 0].numpy(), cmap='gray')\n",
    "        label_text = 'Healthy' if labels_batch[i] == 0 else 'Fractured'\n",
    "        ax.set_title(f'{label_text} ({labels_batch[i].item()})')\n",
    "        ax.axis('off')\n",
    "plt.suptitle('Sample Training Images')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del temp_loader  # Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Understanding the data structure\n",
    "# Each file is CENTERED on the target vertebra from preprocessing\n",
    "\n",
    "nii_files = list(Path(CT_FOLDER).glob('*.nii.gz')) + list(Path(CT_FOLDER).glob('*.nii'))\n",
    "if len(nii_files) > 0:\n",
    "    sample_ct_path = nii_files[0]\n",
    "    ct_vol = nib.load(str(sample_ct_path)).get_fdata()\n",
    "\n",
    "    # Get center slice (where target vertebra should be)\n",
    "    z_center = ct_vol.shape[2] // 2\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Middle slice (center of z-axis = center of target vertebra)\n",
    "    axes[0].imshow(ct_vol[:, :, z_center].T, cmap='gray', origin='lower')\n",
    "    axes[0].set_title(f'Center Slice (z={z_center})\\nTarget vertebra centered here')\n",
    "    axes[0].axhline(y=ct_vol.shape[1]//2, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[0].axvline(x=ct_vol.shape[0]//2, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Earlier slice (may show more of target vertebra body)\n",
    "    z_early = max(0, z_center - 10)\n",
    "    axes[1].imshow(ct_vol[:, :, z_early].T, cmap='gray', origin='lower')\n",
    "    axes[1].set_title(f'Slice z={z_early}')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Later slice\n",
    "    z_late = min(ct_vol.shape[2] - 1, z_center + 10)\n",
    "    axes[2].imshow(ct_vol[:, :, z_late].T, cmap='gray', origin='lower')\n",
    "    axes[2].set_title(f'Slice z={z_late}')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.suptitle(f'Volume: {sample_ct_path.name}\\nShape: {ct_vol.shape} (target vertebra = center)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nFile: {sample_ct_path.name}\")\n",
    "    print(f\"Volume shape: {ct_vol.shape}\")\n",
    "    print(f\"Center of volume = center of target vertebra\")\n",
    "    print(f\"No external localization needed - preprocessing already centered the data\")\n",
    "else:\n",
    "    print(\"No .nii or .nii.gz files found in CT_FOLDER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5b9be",
   "metadata": {},
   "source": [
    "## 7. Model Definition\n",
    "\n",
    "**IMPORTANT**: Must match the architecture in `grad_CAM_3d_sagittal.py`:\n",
    "```python\n",
    "model = SEresnet50(spatial_dims=2, in_channels=1, num_classes=2)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56928155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create SEResNet50 model matching grad_CAM requirements.\"\"\"\n",
    "    model = SEResNet50(\n",
    "        spatial_dims=2,      # 2D images\n",
    "        in_channels=1,       # Grayscale CT\n",
    "        num_classes=2        # Binary: healthy/fractured\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model: SEResNet50\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df294e2",
   "metadata": {},
   "source": [
    "## 8. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c826f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "all_labels = [l for _, _, l in full_dataset.samples]\n",
    "n_healthy = sum(1 for l in all_labels if l == 0)\n",
    "n_fractured = sum(1 for l in all_labels if l == 1)\n",
    "\n",
    "# Inverse frequency weighting\n",
    "if n_fractured > 0 and n_healthy > 0:\n",
    "    weight_healthy = len(all_labels) / (2 * n_healthy)\n",
    "    weight_fractured = len(all_labels) / (2 * n_fractured)\n",
    "    class_weights = torch.tensor([weight_healthy, weight_fractured], dtype=torch.float32).to(device)\n",
    "else:\n",
    "    class_weights = None\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55da83",
   "metadata": {},
   "source": [
    "## 9. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fffe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    return epoch_loss, metrics, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4394dd",
   "metadata": {},
   "source": [
    "## 10. K-Fold Training Loop\n",
    "\n",
    "Train model on each fold and track performance across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eacff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting {N_FOLDS}-Fold Cross-Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Iterate through each fold\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train samples: {len(train_ids)}, Val samples: {len(val_ids)}\")\n",
    "    \n",
    "    # Create data loaders for this fold\n",
    "    train_subset = Subset(full_dataset, train_ids)\n",
    "    val_subset = Subset(full_dataset, val_ids)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Initialize fresh model for this fold\n",
    "    model = SEResNet50(\n",
    "        spatial_dims=2,\n",
    "        in_channels=1,\n",
    "        num_classes=2,\n",
    "        pretrained=False\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss, optimizer, scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "    \n",
    "    # Training history for this fold\n",
    "    fold_history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [],\n",
    "        'val_f1': [], 'val_precision': [], 'val_recall': []\n",
    "    }\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Train for NUM_EPOCHS\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_metrics, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log\n",
    "        fold_history['train_loss'].append(train_loss)\n",
    "        fold_history['train_acc'].append(train_acc)\n",
    "        fold_history['val_loss'].append(val_loss)\n",
    "        fold_history['val_acc'].append(val_metrics['accuracy'])\n",
    "        fold_history['val_f1'].append(val_metrics['f1'])\n",
    "        fold_history['val_precision'].append(val_metrics['precision'])\n",
    "        fold_history['val_recall'].append(val_metrics['recall'])\n",
    "        \n",
    "        # Print every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Val Acc: {val_metrics['accuracy']:.4f} | F1: {val_metrics['f1']:.4f}\")\n",
    "        \n",
    "        # Save best model for this fold\n",
    "        if val_metrics['f1'] > best_f1:\n",
    "            best_f1 = val_metrics['f1']\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            checkpoint = {\n",
    "                'fold': fold + 1,\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'best_f1': best_f1,\n",
    "                'history': fold_history\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(CHECKPOINT_FOLDER, f'fold_{fold+1}_best.tar'))\n",
    "    \n",
    "    # Load best model and evaluate on validation set\n",
    "    checkpoint = torch.load(os.path.join(CHECKPOINT_FOLDER, f'fold_{fold+1}_best.tar'), weights_only=False)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    val_loss, val_metrics, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "    cm = confusion_matrix(val_labels, val_preds)\n",
    "    \n",
    "    # Store fold results\n",
    "    fold_results['train_loss'].append(fold_history['train_loss'][-1])\n",
    "    fold_results['val_loss'].append(val_loss)\n",
    "    fold_results['accuracy'].append(val_metrics['accuracy'])\n",
    "    fold_results['precision'].append(val_metrics['precision'])\n",
    "    fold_results['recall'].append(val_metrics['recall'])\n",
    "    fold_results['f1'].append(val_metrics['f1'])\n",
    "    fold_results['confusion_matrices'].append(cm)\n",
    "    \n",
    "    print(f\"\\nFold {fold + 1} Results:\")\n",
    "    print(f\"  Best Epoch: {best_epoch}\")\n",
    "    print(f\"  Accuracy:  {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {val_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {val_metrics['f1']:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\")\n",
    "    print(f\"    [[{cm[0,0]:4d} {cm[0,1]:4d}]\")\n",
    "    print(f\"     [{cm[1,0]:4d} {cm[1,1]:4d}]]\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"K-FOLD CROSS-VALIDATION COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b652ea9",
   "metadata": {},
   "source": [
    "## 11. Cross-Validation Results Summary\n",
    "\n",
    "Calculate mean and standard deviation across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std for each metric\n",
    "results_summary = {\n",
    "    'Accuracy': (np.mean(fold_results['accuracy']), np.std(fold_results['accuracy'])),\n",
    "    'Precision': (np.mean(fold_results['precision']), np.std(fold_results['precision'])),\n",
    "    'Recall': (np.mean(fold_results['recall']), np.std(fold_results['recall'])),\n",
    "    'F1 Score': (np.mean(fold_results['f1']), np.std(fold_results['f1'])),\n",
    "    'Val Loss': (np.mean(fold_results['val_loss']), np.std(fold_results['val_loss']))\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-VALIDATION RESULTS (Mean ± Std)\")\n",
    "print(\"=\"*80)\n",
    "for metric, (mean, std) in results_summary.items():\n",
    "    print(f\"{metric:12s}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "# Per-fold detailed results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-FOLD DETAILED RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Fold':<6} {'Accuracy':<10} {'Precision':<11} {'Recall':<10} {'F1 Score':<10}\")\n",
    "print(\"-\"*80)\n",
    "for i in range(N_FOLDS):\n",
    "    print(f\"{i+1:<6} {fold_results['accuracy'][i]:<10.4f} {fold_results['precision'][i]:<11.4f} \"\n",
    "          f\"{fold_results['recall'][i]:<10.4f} {fold_results['f1'][i]:<10.4f}\")\n",
    "\n",
    "# Average confusion matrix\n",
    "avg_cm = np.mean(fold_results['confusion_matrices'], axis=0).astype(int)\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"AVERAGE CONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "print(f\"              Predicted 0   Predicted 1\")\n",
    "print(f\"Actual 0:     {avg_cm[0,0]:6d}        {avg_cm[0,1]:6d}\")\n",
    "print(f\"Actual 1:     {avg_cm[1,0]:6d}        {avg_cm[1,1]:6d}\")\n",
    "\n",
    "# Save results to file\n",
    "with open(os.path.join(CHECKPOINT_FOLDER, 'kfold_results.txt'), 'w') as f:\n",
    "    f.write(f\"{N_FOLDS}-Fold Cross-Validation Results\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(\"Mean ± Std:\\n\")\n",
    "    for metric, (mean, std) in results_summary.items():\n",
    "        f.write(f\"{metric:12s}: {mean:.4f} ± {std:.4f}\\n\")\n",
    "    f.write(\"\\nPer-Fold Results:\\n\")\n",
    "    f.write(f\"{'Fold':<6} {'Accuracy':<10} {'Precision':<11} {'Recall':<10} {'F1 Score':<10}\\n\")\n",
    "    for i in range(N_FOLDS):\n",
    "        f.write(f\"{i+1:<6} {fold_results['accuracy'][i]:<10.4f} {fold_results['precision'][i]:<11.4f} \"\n",
    "                f\"{fold_results['recall'][i]:<10.4f} {fold_results['f1'][i]:<10.4f}\\n\")\n",
    "\n",
    "print(f\"\\nResults saved to: {os.path.join(CHECKPOINT_FOLDER, 'kfold_results.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1c51d",
   "metadata": {},
   "source": [
    "## 12. Visualize Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ee968",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Metrics across folds\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics_to_plot, colors)):\n",
    "    ax = axes[0, 0]\n",
    "    values = fold_results[metric]\n",
    "    mean = np.mean(values)\n",
    "    x = np.arange(1, N_FOLDS + 1)\n",
    "    ax.bar(x + i*0.2 - 0.3, values, width=0.2, label=metric.capitalize(), color=color, alpha=0.8)\n",
    "\n",
    "axes[0, 0].axhline(y=np.mean(fold_results['accuracy']), color='gray', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].set_xlabel('Fold')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Metrics Across Folds')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xticks(np.arange(1, N_FOLDS + 1))\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot of metrics\n",
    "axes[0, 1].boxplot([fold_results[m] for m in metrics_to_plot], \n",
    "                    labels=[m.capitalize() for m in metrics_to_plot],\n",
    "                    patch_artist=True)\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_title('Metric Distribution Across Folds')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Average Confusion Matrix\n",
    "import seaborn as sns\n",
    "sns.heatmap(avg_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
    "            xticklabels=['Healthy', 'Fractured'],\n",
    "            yticklabels=['Healthy', 'Fractured'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "axes[1, 0].set_title('Average Confusion Matrix')\n",
    "\n",
    "# 4. F1 Score per fold with error bars\n",
    "x = np.arange(1, N_FOLDS + 1)\n",
    "f1_mean = np.mean(fold_results['f1'])\n",
    "f1_std = np.std(fold_results['f1'])\n",
    "axes[1, 1].bar(x, fold_results['f1'], color='#9b59b6', alpha=0.7, label='Per-fold F1')\n",
    "axes[1, 1].axhline(y=f1_mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {f1_mean:.4f}')\n",
    "axes[1, 1].axhline(y=f1_mean + f1_std, color='red', linestyle=':', alpha=0.5)\n",
    "axes[1, 1].axhline(y=f1_mean - f1_std, color='red', linestyle=':', alpha=0.5)\n",
    "axes[1, 1].fill_between([0.5, N_FOLDS + 0.5], f1_mean - f1_std, f1_mean + f1_std, \n",
    "                         color='red', alpha=0.1, label=f'±1 Std: {f1_std:.4f}')\n",
    "axes[1, 1].set_xlabel('Fold')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].set_title('F1 Score Across Folds')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_FOLDER, 'kfold_results.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualization saved to: {os.path.join(CHECKPOINT_FOLDER, 'kfold_results.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289b110",
   "metadata": {},
   "source": [
    "## 13. Save Best Fold Model for Grad-CAM\n",
    "\n",
    "Choose the best performing fold and save in format compatible with `grad_CAM_3d_sagittal.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best fold based on F1 score\n",
    "best_fold_idx = np.argmax(fold_results['f1'])\n",
    "best_fold_num = best_fold_idx + 1\n",
    "best_fold_f1 = fold_results['f1'][best_fold_idx]\n",
    "\n",
    "print(f\"Best performing fold: Fold {best_fold_num}\")\n",
    "print(f\"Best F1 Score: {best_fold_f1:.4f}\")\n",
    "print(f\"Accuracy: {fold_results['accuracy'][best_fold_idx]:.4f}\")\n",
    "print(f\"Precision: {fold_results['precision'][best_fold_idx]:.4f}\")\n",
    "print(f\"Recall: {fold_results['recall'][best_fold_idx]:.4f}\")\n",
    "\n",
    "# Load best fold model\n",
    "best_model_path = os.path.join(CHECKPOINT_FOLDER, f'fold_{best_fold_num}_best.tar')\n",
    "checkpoint = torch.load(best_model_path, weights_only=False)\n",
    "\n",
    "# Initialize fresh model and load state\n",
    "best_model = SEResNet50(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    num_classes=2,\n",
    "    pretrained=False\n",
    ").to(device)\n",
    "best_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "print(f\"\\nLoaded best fold model from: {best_model_path}\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  F1 Score: {checkpoint['best_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074bb4ca",
   "metadata": {},
   "source": [
    "## 14. Save Model for Grad-CAM\n",
    "\n",
    "Save in format compatible with `grad_CAM_3d_sagittal.py`:\n",
    "```python\n",
    "model = SEresnet50(spatial_dims=2, in_channels=1, num_classes=2)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap model in DataParallel to match grad_CAM expectations\n",
    "model_dp = nn.DataParallel(best_model)\n",
    "\n",
    "# Save checkpoint in expected format\n",
    "gradcam_checkpoint = {\n",
    "    'fold': best_fold_num,\n",
    "    'epoch': checkpoint['epoch'],\n",
    "    'state_dict': model_dp.state_dict(),  # DataParallel state dict\n",
    "    'best_f1': best_fold_f1,\n",
    "    'mean_f1': np.mean(fold_results['f1']),\n",
    "    'std_f1': np.std(fold_results['f1'])\n",
    "}\n",
    "\n",
    "save_path = os.path.join(CHECKPOINT_FOLDER, 'seresnet50_classifier_best.tar')\n",
    "torch.save(gradcam_checkpoint, save_path)\n",
    "print(f\"\\nGrad-CAM compatible checkpoint saved to:\")\n",
    "print(f\"  {save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"USAGE WITH GRAD-CAM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"python Attention/grad_CAM_3d_sagittal.py \\\\\")\n",
    "print(f\"  --ckpt-path {save_path} \\\\\")\n",
    "print(f\"  --dataroot <straightened_CT_folder> \\\\\")\n",
    "print(f\"  --output-folder <heatmap_output_folder>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9697c9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Training Approach\n",
    "- **Method**: 5-Fold Cross-Validation\n",
    "- **Purpose**: Robust evaluation and reduced overfitting\n",
    "- **Each fold**: Separate model trained independently\n",
    "\n",
    "### Model Architecture\n",
    "- **Model**: MONAI SEResNet50\n",
    "- **Input**: 2D grayscale images (1 channel, 64x64)\n",
    "- **Output**: 2 classes (healthy/fractured)\n",
    "- **Spatial dims**: 2D\n",
    "\n",
    "### Data Pipeline\n",
    "1. Load 3D straightened vertebra volume (`.nii` or `.nii.gz`)\n",
    "2. Extract middle 30 slices (center ± 15)\n",
    "3. Each slice inherits parent vertebra's label\n",
    "4. Normalize to [0, 1]\n",
    "5. Binary classification: 0=healthy, 1+=fractured\n",
    "\n",
    "### Ground Truth\n",
    "- File: `vertebra_data.json`\n",
    "- Format: `{\"patient_ct_vertebraID\": genant_grade}`\n",
    "- Mapping: Grade 0 → Class 0, Grade 1/2/3 → Class 1\n",
    "\n",
    "### Output Files\n",
    "- **Per-fold checkpoints**: `fold_1_best.tar` through `fold_5_best.tar`\n",
    "- **Best model**: `seresnet50_classifier_best.tar` (best fold, Grad-CAM compatible)\n",
    "- **Results**: `kfold_results.txt` (detailed metrics)\n",
    "- **Visualization**: `kfold_results.png` (plots)\n",
    "\n",
    "### Cross-Validation Benefits\n",
    "- More robust performance estimate\n",
    "- Reduces variance from single train/val split\n",
    "- Uses all data for both training and validation\n",
    "- Identifies model stability across different data subsets"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
