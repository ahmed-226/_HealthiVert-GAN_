{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f958e810",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff41f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install monai nibabel torch torchvision scikit-learn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from monai.networks.nets import SEResNet50\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce5076",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== MODIFY THESE PATHS ==============\n",
    "# Path to straightened CT volumes\n",
    "CT_FOLDER = \"/content/verse19/straighten/CT\"  # Colab path\n",
    "# CT_FOLDER = \"d:/Graduation Project/HeathiVert/verse19/straighten/CT\"  # Windows path\n",
    "\n",
    "# Path to vertebra_data.json with ground truth labels\n",
    "JSON_PATH = \"/content/verse19/vertebra_data_test.json\"  # Colab path  \n",
    "# JSON_PATH = \"d:/Graduation Project/HeathiVert/verse19/vertebra_data_test.json\"  # Windows path\n",
    "\n",
    "# Output folder for checkpoints\n",
    "CHECKPOINT_FOLDER = \"/content/checkpoints/classifier\"  # Colab path\n",
    "# CHECKPOINT_FOLDER = \"d:/Graduation Project/HeathiVert/checkpoints/classifier\"  # Windows path\n",
    "\n",
    "# ============== TRAINING HYPERPARAMETERS ==============\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NUM_SLICES = 30  # Number of slices to extract from each volume (center ± 15)\n",
    "VAL_SPLIT = 0.2  # Validation split ratio\n",
    "\n",
    "# Create checkpoint folder\n",
    "os.makedirs(CHECKPOINT_FOLDER, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a23b04",
   "metadata": {},
   "source": [
    "## 3. Load Ground Truth Labels\n",
    "\n",
    "The JSON structure is:\n",
    "```json\n",
    "{\n",
    "  \"train\": {\"sub-verse004_ct_23\": 0, \"sub-verse020_12\": 1, ...},\n",
    "  \"test\": {...},\n",
    "  \"val\": {...}\n",
    "}\n",
    "```\n",
    "\n",
    "We convert Genant grades (0,1,2,3) to binary (0=healthy, 1=fractured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10404cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(json_path):\n",
    "    \"\"\"Load vertebra labels from JSON and convert to binary classification.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Combine all splits\n",
    "    all_labels = {}\n",
    "    for split in ['train', 'test', 'val']:\n",
    "        if split in data:\n",
    "            all_labels.update(data[split])\n",
    "    \n",
    "    # Convert to binary: 0 = healthy, 1+ = fractured\n",
    "    binary_labels = {k: (1 if v > 0 else 0) for k, v in all_labels.items()}\n",
    "    \n",
    "    # Statistics\n",
    "    n_healthy = sum(1 for v in binary_labels.values() if v == 0)\n",
    "    n_fractured = sum(1 for v in binary_labels.values() if v == 1)\n",
    "    \n",
    "    print(f\"Total vertebrae: {len(binary_labels)}\")\n",
    "    print(f\"  Healthy (0): {n_healthy} ({100*n_healthy/len(binary_labels):.1f}%)\")\n",
    "    print(f\"  Fractured (1): {n_fractured} ({100*n_fractured/len(binary_labels):.1f}%)\")\n",
    "    \n",
    "    return binary_labels\n",
    "\n",
    "labels = load_labels(JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9255ec1",
   "metadata": {},
   "source": [
    "## 4. Dataset Class\n",
    "\n",
    "**Key Points:**\n",
    "- Each `.nii.gz` file is a single straightened vertebra (not whole spine)\n",
    "- File naming: `sub-verse004_ct_23.nii.gz` → lookup key: `sub-verse004_ct_23`\n",
    "- Extract **middle 30 slices** (z_center ± 15) as 2D images\n",
    "- Each slice gets the same label as its parent vertebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertebraSliceDataset(Dataset):\n",
    "    \"\"\"Dataset that extracts 2D slices from 3D straightened vertebra volumes.\"\"\"\n",
    "    \n",
    "    def __init__(self, ct_folder, labels_dict, num_slices=30, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ct_folder: Path to folder containing .nii.gz files\n",
    "            labels_dict: Dictionary mapping vertebra_id -> binary label\n",
    "            num_slices: Number of slices to extract from each volume (center ± num_slices/2)\n",
    "            transform: Optional transforms to apply to each slice\n",
    "        \"\"\"\n",
    "        self.ct_folder = Path(ct_folder)\n",
    "        self.labels_dict = labels_dict\n",
    "        self.num_slices = num_slices\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Find all .nii.gz files and their labels\n",
    "        self.samples = []  # List of (nii_path, slice_idx, label)\n",
    "        self._prepare_samples()\n",
    "        \n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Build list of (file_path, slice_index, label) tuples.\"\"\"\n",
    "        nii_files = list(self.ct_folder.glob('*.nii.gz'))\n",
    "        print(f\"Found {len(nii_files)} .nii.gz files\")\n",
    "        \n",
    "        matched = 0\n",
    "        unmatched = []\n",
    "        \n",
    "        for nii_path in nii_files:\n",
    "            # Extract vertebra ID from filename\n",
    "            # e.g., \"sub-verse004_ct_23.nii.gz\" -> \"sub-verse004_ct_23\"\n",
    "            vertebra_id = nii_path.stem.replace('.nii', '')\n",
    "            \n",
    "            # Lookup label\n",
    "            if vertebra_id in self.labels_dict:\n",
    "                label = self.labels_dict[vertebra_id]\n",
    "                matched += 1\n",
    "                \n",
    "                # Load volume to get dimensions\n",
    "                try:\n",
    "                    vol = nib.load(str(nii_path))\n",
    "                    z_dim = vol.shape[2]\n",
    "                    z_center = z_dim // 2\n",
    "                    half_slices = self.num_slices // 2\n",
    "                    \n",
    "                    # Extract middle slices\n",
    "                    start = max(0, z_center - half_slices)\n",
    "                    end = min(z_dim, z_center + half_slices)\n",
    "                    \n",
    "                    for slice_idx in range(start, end):\n",
    "                        self.samples.append((nii_path, slice_idx, label))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {nii_path}: {e}\")\n",
    "            else:\n",
    "                unmatched.append(vertebra_id)\n",
    "        \n",
    "        print(f\"Matched: {matched} vertebrae\")\n",
    "        print(f\"Unmatched: {len(unmatched)} vertebrae\")\n",
    "        print(f\"Total slices: {len(self.samples)}\")\n",
    "        \n",
    "        # Class distribution\n",
    "        n_healthy = sum(1 for _, _, l in self.samples if l == 0)\n",
    "        n_fractured = sum(1 for _, _, l in self.samples if l == 1)\n",
    "        print(f\"Slice distribution: Healthy={n_healthy}, Fractured={n_fractured}\")\n",
    "        \n",
    "        if unmatched[:5]:\n",
    "            print(f\"Sample unmatched IDs: {unmatched[:5]}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        nii_path, slice_idx, label = self.samples[idx]\n",
    "        \n",
    "        # Load volume and extract slice\n",
    "        vol = nib.load(str(nii_path)).get_fdata()\n",
    "        slice_2d = vol[:, :, slice_idx].astype(np.float32)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        slice_min, slice_max = slice_2d.min(), slice_2d.max()\n",
    "        if slice_max > slice_min:\n",
    "            slice_2d = (slice_2d - slice_min) / (slice_max - slice_min)\n",
    "        \n",
    "        # Add channel dimension: (H, W) -> (1, H, W)\n",
    "        slice_2d = np.expand_dims(slice_2d, axis=0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        slice_tensor = torch.from_numpy(slice_2d)\n",
    "        \n",
    "        if self.transform:\n",
    "            slice_tensor = self.transform(slice_tensor)\n",
    "        \n",
    "        return slice_tensor, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2227f6b",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a27622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full dataset\n",
    "full_dataset = VertebraSliceDataset(\n",
    "    ct_folder=CT_FOLDER,\n",
    "    labels_dict=labels,\n",
    "    num_slices=NUM_SLICES,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int((1 - VAL_SPLIT) * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e4d48",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch\n",
    "images, labels_batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Labels: {labels_batch[:8]}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(images):\n",
    "        ax.imshow(images[i, 0].numpy(), cmap='gray')\n",
    "        label_text = 'Healthy' if labels_batch[i] == 0 else 'Fractured'\n",
    "        ax.set_title(f'{label_text} ({labels_batch[i].item()})')\n",
    "        ax.axis('off')\n",
    "plt.suptitle('Sample Training Images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Understanding the data structure\n",
    "# Each file is CENTERED on the target vertebra from preprocessing\n",
    "\n",
    "sample_ct_path = list(Path(CT_FOLDER).glob('*.nii.gz'))[0]\n",
    "ct_vol = nib.load(str(sample_ct_path)).get_fdata()\n",
    "\n",
    "# Get center slice (where target vertebra should be)\n",
    "z_center = ct_vol.shape[2] // 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Middle slice (center of z-axis = center of target vertebra)\n",
    "axes[0].imshow(ct_vol[:, :, z_center].T, cmap='gray', origin='lower')\n",
    "axes[0].set_title(f'Center Slice (z={z_center})\\nTarget vertebra centered here')\n",
    "axes[0].axhline(y=ct_vol.shape[1]//2, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=ct_vol.shape[0]//2, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Earlier slice (may show more of target vertebra body)\n",
    "z_early = z_center - 10\n",
    "axes[1].imshow(ct_vol[:, :, z_early].T, cmap='gray', origin='lower')\n",
    "axes[1].set_title(f'Slice z={z_early}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Later slice\n",
    "z_late = z_center + 10\n",
    "axes[2].imshow(ct_vol[:, :, z_late].T, cmap='gray', origin='lower')\n",
    "axes[2].set_title(f'Slice z={z_late}')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(f'Volume: {sample_ct_path.name}\\nShape: {ct_vol.shape} (target vertebra = center)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFile: {sample_ct_path.name}\")\n",
    "print(f\"Volume shape: {ct_vol.shape}\")\n",
    "print(f\"Center of volume = center of target vertebra\")\n",
    "print(f\"No external localization needed - preprocessing already centered the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5b9be",
   "metadata": {},
   "source": [
    "## 7. Model Definition\n",
    "\n",
    "**IMPORTANT**: Must match the architecture in `grad_CAM_3d_sagittal.py`:\n",
    "```python\n",
    "model = SEresnet50(spatial_dims=2, in_channels=1, num_classes=2)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56928155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create SEResNet50 model matching grad_CAM requirements.\"\"\"\n",
    "    model = SEResNet50(\n",
    "        spatial_dims=2,      # 2D images\n",
    "        in_channels=1,       # Grayscale CT\n",
    "        num_classes=2        # Binary: healthy/fractured\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model: SEResNet50\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df294e2",
   "metadata": {},
   "source": [
    "## 8. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c826f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "all_labels = [l for _, _, l in full_dataset.samples]\n",
    "n_healthy = sum(1 for l in all_labels if l == 0)\n",
    "n_fractured = sum(1 for l in all_labels if l == 1)\n",
    "\n",
    "# Inverse frequency weighting\n",
    "if n_fractured > 0 and n_healthy > 0:\n",
    "    weight_healthy = len(all_labels) / (2 * n_healthy)\n",
    "    weight_fractured = len(all_labels) / (2 * n_fractured)\n",
    "    class_weights = torch.tensor([weight_healthy, weight_fractured], dtype=torch.float32).to(device)\n",
    "else:\n",
    "    class_weights = None\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55da83",
   "metadata": {},
   "source": [
    "## 9. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fffe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    return epoch_loss, metrics, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4394dd",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eacff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'val_f1': [], 'val_precision': [], 'val_recall': []\n",
    "}\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_precision'].append(val_metrics['precision'])\n",
    "    history['val_recall'].append(val_metrics['recall'])\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Val F1: {val_metrics['f1']:.4f} | Precision: {val_metrics['precision']:.4f} | Recall: {val_metrics['recall']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_f1': best_f1,\n",
    "            'history': history\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(CHECKPOINT_FOLDER, 'best_model.tar'))\n",
    "        print(f\"  ✓ Saved best model (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'history': history\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(CHECKPOINT_FOLDER, f'checkpoint_epoch_{epoch+1}.tar'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Best F1: {best_f1:.4f} at epoch {best_epoch}\")\n",
    "print(f\"Checkpoints saved to: {CHECKPOINT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b652ea9",
   "metadata": {},
   "source": [
    "## 11. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# F1, Precision, Recall\n",
    "axes[2].plot(history['val_f1'], label='F1')\n",
    "axes[2].plot(history['val_precision'], label='Precision')\n",
    "axes[2].plot(history['val_recall'], label='Recall')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Score')\n",
    "axes[2].set_title('Validation Metrics')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_FOLDER, 'training_curves.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289b110",
   "metadata": {},
   "source": [
    "## 12. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(CHECKPOINT_FOLDER, 'best_model.tar'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "\n",
    "# Final validation\n",
    "val_loss, val_metrics, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {val_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {val_metrics['f1']:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"              Pred 0   Pred 1\")\n",
    "print(f\"Actual 0:     {cm[0,0]:5d}    {cm[0,1]:5d}\")\n",
    "print(f\"Actual 1:     {cm[1,0]:5d}    {cm[1,1]:5d}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=['Healthy', 'Fractured']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074bb4ca",
   "metadata": {},
   "source": [
    "## 13. Save Model for Grad-CAM\n",
    "\n",
    "Save in format compatible with `grad_CAM_3d_sagittal.py`:\n",
    "```python\n",
    "model = SEresnet50(spatial_dims=2, in_channels=1, num_classes=2)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap model in DataParallel to match grad_CAM expectations\n",
    "model_dp = nn.DataParallel(model)\n",
    "\n",
    "# Save checkpoint in expected format\n",
    "gradcam_checkpoint = {\n",
    "    'epoch': checkpoint['epoch'],\n",
    "    'state_dict': model_dp.state_dict(),  # DataParallel state dict\n",
    "    'best_f1': best_f1,\n",
    "    'metrics': val_metrics\n",
    "}\n",
    "\n",
    "save_path = os.path.join(CHECKPOINT_FOLDER, 'seresnet50_classifier.tar')\n",
    "torch.save(gradcam_checkpoint, save_path)\n",
    "print(f\"\\nGrad-CAM compatible checkpoint saved to:\")\n",
    "print(f\"  {save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"USAGE WITH GRAD-CAM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"python Attention/grad_CAM_3d_sagittal.py \\\\\")\n",
    "print(f\"  --ckpt-path {save_path} \\\\\")\n",
    "print(f\"  --dataroot <straightened_CT_folder> \\\\\")\n",
    "print(f\"  --output-folder <heatmap_output_folder>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0f955",
   "metadata": {},
   "source": [
    "## 14. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20752a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Healthy', 'Fractured'],\n",
    "            yticklabels=['Healthy', 'Fractured'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix\\nAccuracy: {val_metrics[\"accuracy\"]:.2%}, F1: {val_metrics[\"f1\"]:.4f}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_FOLDER, 'confusion_matrix.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9697c9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Architecture\n",
    "- **Model**: MONAI SEResNet50\n",
    "- **Input**: 2D grayscale images (1 channel)\n",
    "- **Output**: 2 classes (healthy/fractured)\n",
    "- **Spatial dims**: 2D\n",
    "\n",
    "### Data Pipeline\n",
    "1. Load 3D straightened vertebra volume (`.nii.gz`)\n",
    "2. Extract middle 30 slices (center ± 15)\n",
    "3. Each slice inherits parent vertebra's label\n",
    "4. Normalize to [0, 1]\n",
    "5. Binary classification: 0=healthy, 1+=fractured\n",
    "\n",
    "### Ground Truth\n",
    "- File: `vertebra_data.json`\n",
    "- Format: `{\"patient_ct_vertebraID\": genant_grade}`\n",
    "- Mapping: Grade 0 → Class 0, Grade 1/2/3 → Class 1\n",
    "\n",
    "### Output\n",
    "- Checkpoint: `seresnet50_classifier.tar`\n",
    "- Compatible with `grad_CAM_3d_sagittal.py`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
