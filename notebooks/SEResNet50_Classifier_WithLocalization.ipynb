{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ed875d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install monai nibabel torch torchvision scikit-learn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from monai.networks.nets import SEResNet50\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806af37d",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== MODIFY THESE PATHS ==============\n",
    "# Path to straightened CT volumes (whole spine, vertebra-centered)\n",
    "CT_FOLDER = \"/content/verse19/straighten/CT\"  # Colab path\n",
    "# CT_FOLDER = \"d:/Graduation Project/HeathiVert/verse19/straighten/CT\"  # Windows path\n",
    "\n",
    "# Path to mask_2d folder (binary masks for target vertebra localization)\n",
    "MASK_FOLDER = \"/content/verse19/straighten/mask_2d\"  # Colab path\n",
    "# MASK_FOLDER = \"d:/Graduation Project/HeathiVert/verse19/straighten/mask_2d\"  # Windows path\n",
    "\n",
    "# Path to vertebra_data.json with ground truth labels\n",
    "JSON_PATH = \"/content/vertebra_data.json\"  # Colab path  \n",
    "# JSON_PATH = \"d:/Graduation Project/HeathiVert/vertebra_data.json\"  # Windows path\n",
    "\n",
    "# Output folder for checkpoints\n",
    "CHECKPOINT_FOLDER = \"/content/checkpoints/classifier_localized\"  # Colab path\n",
    "# CHECKPOINT_FOLDER = \"d:/Graduation Project/HeathiVert/checkpoints/classifier_localized\"  # Windows path\n",
    "\n",
    "# ============== TRAINING HYPERPARAMETERS ==============\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NUM_SLICES = 30  # Number of slices to extract from each volume\n",
    "VAL_SPLIT = 0.2  # Validation split ratio\n",
    "CROP_SIZE = 128  # Size to crop around vertebra bounding box\n",
    "\n",
    "# Create checkpoint folder\n",
    "os.makedirs(CHECKPOINT_FOLDER, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_FOLDER}\")\n",
    "print(f\"\\nAPPROACH: WITH MASK LOCALIZATION (crop {CROP_SIZE}x{CROP_SIZE} around target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92d35b",
   "metadata": {},
   "source": [
    "## 3. Load Ground Truth Labels\n",
    "\n",
    "The JSON structure is:\n",
    "```json\n",
    "{\n",
    "  \"train\": {\"sub-verse004_23\": 0, \"sub-verse020_12\": 1, ...},\n",
    "  \"test\": {...},\n",
    "  \"val\": {...}\n",
    "}\n",
    "```\n",
    "\n",
    "We convert Genant grades (0,1,2,3) to binary (0=healthy, 1=fractured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(json_path):\n",
    "    \"\"\"Load vertebra labels from JSON and convert to binary classification.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Combine all splits\n",
    "    all_labels = {}\n",
    "    for split in ['train', 'test', 'val']:\n",
    "        if split in data:\n",
    "            all_labels.update(data[split])\n",
    "    \n",
    "    # Convert to binary: 0 = healthy, 1+ = fractured\n",
    "    binary_labels = {k: (1 if v > 0 else 0) for k, v in all_labels.items()}\n",
    "    \n",
    "    # Statistics\n",
    "    n_healthy = sum(1 for v in binary_labels.values() if v == 0)\n",
    "    n_fractured = sum(1 for v in binary_labels.values() if v == 1)\n",
    "    \n",
    "    print(f\"Total vertebrae: {len(binary_labels)}\")\n",
    "    print(f\"  Healthy (0): {n_healthy} ({100*n_healthy/len(binary_labels):.1f}%)\")\n",
    "    print(f\"  Fractured (1): {n_fractured} ({100*n_fractured/len(binary_labels):.1f}%)\")\n",
    "    \n",
    "    return binary_labels\n",
    "\n",
    "labels = load_labels(JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d09cde",
   "metadata": {},
   "source": [
    "## 4. Dataset Class WITH Mask-Based Localization\n",
    "\n",
    "**Key Points:**\n",
    "- Each CT file contains the **whole straightened spine** (multiple vertebrae)\n",
    "- The `mask_2d` folder contains a **binary mask** isolating only the target vertebra\n",
    "- We find the **bounding box** of the target vertebra from the mask\n",
    "- Extract 2D slices **cropped around the vertebra center** (128×128)\n",
    "- This ensures the classifier sees ONLY ONE vertebra per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e44513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertebraSliceDatasetLocalized(Dataset):\n",
    "    \"\"\"Dataset that extracts 2D slices using mask_2d for localization.\n",
    "    \n",
    "    Uses binary mask to find the target vertebra bounding box and crops\n",
    "    a fixed-size region around it. This ensures the model sees ONLY the\n",
    "    target vertebra, not neighboring ones.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ct_folder, mask_folder, labels_dict, num_slices=30, crop_size=128, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ct_folder: Path to folder containing CT .nii.gz files (whole spine)\n",
    "            mask_folder: Path to folder containing mask_2d .nii.gz files (target vertebra mask)\n",
    "            labels_dict: Dictionary mapping vertebra_id -> binary label\n",
    "            num_slices: Number of slices to extract from each volume\n",
    "            crop_size: Size of the square crop around vertebra center\n",
    "            transform: Optional transforms to apply to each slice\n",
    "        \"\"\"\n",
    "        self.ct_folder = Path(ct_folder)\n",
    "        self.mask_folder = Path(mask_folder)\n",
    "        self.labels_dict = labels_dict\n",
    "        self.num_slices = num_slices\n",
    "        self.crop_size = crop_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Cache for bounding boxes to avoid recomputing\n",
    "        self.bbox_cache = {}\n",
    "        \n",
    "        # Find all .nii.gz files and their labels\n",
    "        self.samples = []  # List of (ct_path, mask_path, slice_idx, label)\n",
    "        self._prepare_samples()\n",
    "        \n",
    "    def _get_vertebra_bbox(self, mask_path):\n",
    "        \"\"\"Get bounding box of vertebra from mask. Returns (y_min, y_max, x_min, x_max, z_min, z_max).\"\"\"\n",
    "        if str(mask_path) in self.bbox_cache:\n",
    "            return self.bbox_cache[str(mask_path)]\n",
    "        \n",
    "        mask = nib.load(str(mask_path)).get_fdata()\n",
    "        # Find non-zero voxels\n",
    "        nonzero = np.where(mask > 0)\n",
    "        \n",
    "        if len(nonzero[0]) == 0:\n",
    "            return None\n",
    "        \n",
    "        bbox = (\n",
    "            nonzero[0].min(), nonzero[0].max(),  # y_min, y_max\n",
    "            nonzero[1].min(), nonzero[1].max(),  # x_min, x_max\n",
    "            nonzero[2].min(), nonzero[2].max()   # z_min, z_max\n",
    "        )\n",
    "        self.bbox_cache[str(mask_path)] = bbox\n",
    "        return bbox\n",
    "        \n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Build list of (ct_path, mask_path, slice_index, label) tuples.\"\"\"\n",
    "        ct_files = list(self.ct_folder.glob('*.nii.gz'))\n",
    "        print(f\"Found {len(ct_files)} CT .nii.gz files\")\n",
    "        \n",
    "        matched = 0\n",
    "        unmatched = []\n",
    "        \n",
    "        for ct_path in ct_files:\n",
    "            # Extract vertebra ID from filename\n",
    "            filename = ct_path.stem.replace('.nii', '')\n",
    "            \n",
    "            # Try different ID formats for matching with JSON\n",
    "            possible_ids = [\n",
    "                filename,                          # sub-verse004_ct_23\n",
    "                filename.replace('_ct_', '_'),     # sub-verse004_23\n",
    "            ]\n",
    "            \n",
    "            label = None\n",
    "            for pid in possible_ids:\n",
    "                if pid in self.labels_dict:\n",
    "                    label = self.labels_dict[pid]\n",
    "                    break\n",
    "            \n",
    "            if label is not None:\n",
    "                # Check if corresponding mask exists\n",
    "                mask_path = self.mask_folder / ct_path.name\n",
    "                if not mask_path.exists():\n",
    "                    print(f\"Warning: Mask not found for {ct_path.name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get bounding box from mask\n",
    "                bbox = self._get_vertebra_bbox(mask_path)\n",
    "                if bbox is None:\n",
    "                    print(f\"Warning: Empty mask for {ct_path.name}\")\n",
    "                    continue\n",
    "                \n",
    "                matched += 1\n",
    "                z_min, z_max = bbox[4], bbox[5]\n",
    "                z_center = (z_min + z_max) // 2\n",
    "                \n",
    "                # Extract slices centered on vertebra's z-range\n",
    "                half_slices = self.num_slices // 2\n",
    "                start = max(z_min, z_center - half_slices)\n",
    "                end = min(z_max + 1, z_center + half_slices)\n",
    "                \n",
    "                for slice_idx in range(start, end):\n",
    "                    self.samples.append((ct_path, mask_path, slice_idx, label))\n",
    "            else:\n",
    "                unmatched.append(filename)\n",
    "        \n",
    "        print(f\"Matched: {matched} vertebrae\")\n",
    "        print(f\"Unmatched: {len(unmatched)} vertebrae\")\n",
    "        print(f\"Total slices: {len(self.samples)}\")\n",
    "        \n",
    "        # Class distribution\n",
    "        n_healthy = sum(1 for _, _, _, l in self.samples if l == 0)\n",
    "        n_fractured = sum(1 for _, _, _, l in self.samples if l == 1)\n",
    "        print(f\"Slice distribution: Healthy={n_healthy}, Fractured={n_fractured}\")\n",
    "        \n",
    "        if unmatched[:5]:\n",
    "            print(f\"Sample unmatched IDs: {unmatched[:5]}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ct_path, mask_path, slice_idx, label = self.samples[idx]\n",
    "        \n",
    "        # Load CT volume and extract slice\n",
    "        ct_vol = nib.load(str(ct_path)).get_fdata()\n",
    "        slice_2d = ct_vol[:, :, slice_idx].astype(np.float32)\n",
    "        \n",
    "        # Get bounding box for cropping\n",
    "        bbox = self.bbox_cache[str(mask_path)]\n",
    "        y_min, y_max, x_min, x_max = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        \n",
    "        # Calculate center of vertebra\n",
    "        y_center = (y_min + y_max) // 2\n",
    "        x_center = (x_min + x_max) // 2\n",
    "        \n",
    "        # Crop around center\n",
    "        half_crop = self.crop_size // 2\n",
    "        y_start = max(0, y_center - half_crop)\n",
    "        y_end = min(slice_2d.shape[0], y_center + half_crop)\n",
    "        x_start = max(0, x_center - half_crop)\n",
    "        x_end = min(slice_2d.shape[1], x_center + half_crop)\n",
    "        \n",
    "        cropped = slice_2d[y_start:y_end, x_start:x_end]\n",
    "        \n",
    "        # Pad if necessary to reach crop_size x crop_size\n",
    "        if cropped.shape[0] < self.crop_size or cropped.shape[1] < self.crop_size:\n",
    "            padded = np.zeros((self.crop_size, self.crop_size), dtype=np.float32)\n",
    "            pad_y = (self.crop_size - cropped.shape[0]) // 2\n",
    "            pad_x = (self.crop_size - cropped.shape[1]) // 2\n",
    "            padded[pad_y:pad_y+cropped.shape[0], pad_x:pad_x+cropped.shape[1]] = cropped\n",
    "            cropped = padded\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        slice_min, slice_max = cropped.min(), cropped.max()\n",
    "        if slice_max > slice_min:\n",
    "            cropped = (cropped - slice_min) / (slice_max - slice_min)\n",
    "        \n",
    "        # Add channel dimension: (H, W) -> (1, H, W)\n",
    "        cropped = np.expand_dims(cropped, axis=0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        slice_tensor = torch.from_numpy(cropped)\n",
    "        \n",
    "        if self.transform:\n",
    "            slice_tensor = self.transform(slice_tensor)\n",
    "        \n",
    "        return slice_tensor, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b370a5",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46257fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full dataset WITH mask-based localization\n",
    "full_dataset = VertebraSliceDatasetLocalized(\n",
    "    ct_folder=CT_FOLDER,\n",
    "    mask_folder=MASK_FOLDER,\n",
    "    labels_dict=labels,\n",
    "    num_slices=NUM_SLICES,\n",
    "    crop_size=CROP_SIZE,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int((1 - VAL_SPLIT) * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312c62e",
   "metadata": {},
   "source": [
    "## 6. Visualize: Full Spine vs Cropped Vertebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Full spine vs Cropped vertebra (localization demo)\n",
    "sample_ct_path = list(Path(CT_FOLDER).glob('*.nii.gz'))[0]\n",
    "sample_mask_path = Path(MASK_FOLDER) / sample_ct_path.name\n",
    "\n",
    "ct_vol = nib.load(str(sample_ct_path)).get_fdata()\n",
    "mask_vol = nib.load(str(sample_mask_path)).get_fdata()\n",
    "\n",
    "# Get center slice\n",
    "z_center = ct_vol.shape[2] // 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Full spine\n",
    "axes[0].imshow(ct_vol[:, :, z_center].T, cmap='gray', origin='lower')\n",
    "axes[0].set_title('Full Straightened Spine (CT)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Mask overlay\n",
    "axes[1].imshow(ct_vol[:, :, z_center].T, cmap='gray', origin='lower')\n",
    "axes[1].imshow(mask_vol[:, :, z_center].T, cmap='Reds', alpha=0.4, origin='lower')\n",
    "axes[1].set_title('With Target Vertebra Mask (mask_2d)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Find bbox and show cropped\n",
    "nonzero = np.where(mask_vol > 0)\n",
    "y_min, y_max = nonzero[0].min(), nonzero[0].max()\n",
    "x_min, x_max = nonzero[1].min(), nonzero[1].max()\n",
    "y_center_v, x_center_v = (y_min + y_max) // 2, (x_min + x_max) // 2\n",
    "\n",
    "half = CROP_SIZE // 2\n",
    "cropped = ct_vol[y_center_v-half:y_center_v+half, x_center_v-half:x_center_v+half, z_center]\n",
    "axes[2].imshow(cropped.T, cmap='gray', origin='lower')\n",
    "axes[2].set_title(f'Cropped Target Vertebra ({CROP_SIZE}x{CROP_SIZE})')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(f'Localization Demo: {sample_ct_path.name}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"File: {sample_ct_path.name}\")\n",
    "print(f\"Full spine shape: {ct_vol.shape}\")\n",
    "print(f\"Vertebra bbox: y=[{y_min}:{y_max}], x=[{x_min}:{x_max}]\")\n",
    "print(f\"Cropped region: {CROP_SIZE}x{CROP_SIZE} centered on target vertebra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and visualize cropped samples\n",
    "images, labels_batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {images.shape}\")  # Should be (batch, 1, 128, 128)\n",
    "print(f\"Labels: {labels_batch[:8]}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(images):\n",
    "        ax.imshow(images[i, 0].numpy(), cmap='gray')\n",
    "        label_text = 'Healthy' if labels_batch[i] == 0 else 'Fractured'\n",
    "        ax.set_title(f'{label_text} ({labels_batch[i].item()})')\n",
    "        ax.axis('off')\n",
    "plt.suptitle(f'Sample Training Images (Cropped {CROP_SIZE}x{CROP_SIZE} - Single Vertebra)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35857f5",
   "metadata": {},
   "source": [
    "## 7. Model Definition\n",
    "\n",
    "**IMPORTANT**: Must match the architecture in `grad_CAM_3d_sagittal.py`:\n",
    "```python\n",
    "model = SEresnet50(spatial_dims=2, in_channels=1, num_classes=2)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89173a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create SEResNet50 model matching grad_CAM requirements.\"\"\"\n",
    "    model = SEResNet50(\n",
    "        spatial_dims=2,      # 2D images\n",
    "        in_channels=1,       # Grayscale CT\n",
    "        num_classes=2        # Binary: healthy/fractured\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model: SEResNet50\")\n",
    "print(f\"Input size: 1 x {CROP_SIZE} x {CROP_SIZE}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b9d8c7",
   "metadata": {},
   "source": [
    "## 8. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "all_labels = [l for _, _, _, l in full_dataset.samples]\n",
    "n_healthy = sum(1 for l in all_labels if l == 0)\n",
    "n_fractured = sum(1 for l in all_labels if l == 1)\n",
    "\n",
    "# Inverse frequency weighting\n",
    "if n_fractured > 0 and n_healthy > 0:\n",
    "    weight_healthy = len(all_labels) / (2 * n_healthy)\n",
    "    weight_fractured = len(all_labels) / (2 * n_fractured)\n",
    "    class_weights = torch.tensor([weight_healthy, weight_fractured], dtype=torch.float32).to(device)\n",
    "else:\n",
    "    class_weights = None\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e06a5c",
   "metadata": {},
   "source": [
    "## 9. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1302213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    return epoch_loss, metrics, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6a97d",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'val_f1': [], 'val_precision': [], 'val_recall': []\n",
    "}\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"APPROACH: WITH MASK LOCALIZATION ({CROP_SIZE}x{CROP_SIZE} crops)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_precision'].append(val_metrics['precision'])\n",
    "    history['val_recall'].append(val_metrics['recall'])\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Val F1: {val_metrics['f1']:.4f} | Precision: {val_metrics['precision']:.4f} | Recall: {val_metrics['recall']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_f1': best_f1,\n",
    "            'history': history,\n",
    "            'crop_size': CROP_SIZE,\n",
    "            'approach': 'localized'\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(CHECKPOINT_FOLDER, 'best_model_localized.tar'))\n",
    "        print(f\"  ✓ Saved best model (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'history': history,\n",
    "            'crop_size': CROP_SIZE,\n",
    "            'approach': 'localized'\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(CHECKPOINT_FOLDER, f'checkpoint_epoch_{epoch+1}.tar'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Best F1: {best_f1:.4f} at epoch {best_epoch}\")\n",
    "print(f\"Checkpoints saved to: {CHECKPOINT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33e0cf",
   "metadata": {},
   "source": [
    "## 11. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# F1, Precision, Recall\n",
    "axes[2].plot(history['val_f1'], label='F1')\n",
    "axes[2].plot(history['val_precision'], label='Precision')\n",
    "axes[2].plot(history['val_recall'], label='Recall')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Score')\n",
    "axes[2].set_title('Validation Metrics')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.suptitle('Training Curves (WITH Localization)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_FOLDER, 'training_curves_localized.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270cf09",
   "metadata": {},
   "source": [
    "## 12. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e041d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(CHECKPOINT_FOLDER, 'best_model_localized.tar'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "\n",
    "# Final validation\n",
    "val_loss, val_metrics, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION RESULTS (WITH LOCALIZATION)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {val_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {val_metrics['f1']:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"              Pred 0   Pred 1\")\n",
    "print(f\"Actual 0:     {cm[0,0]:5d}    {cm[0,1]:5d}\")\n",
    "print(f\"Actual 1:     {cm[1,0]:5d}    {cm[1,1]:5d}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=['Healthy', 'Fractured']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec872d",
   "metadata": {},
   "source": [
    "## 13. Save Model for Grad-CAM\n",
    "\n",
    "**NOTE**: For Grad-CAM inference with this localized model, you'll need to:\n",
    "1. Load the mask_2d for each CT file\n",
    "2. Crop the CT around the target vertebra\n",
    "3. Run inference on the cropped image\n",
    "4. Map the heatmap back to the original coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958964df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap model in DataParallel to match grad_CAM expectations\n",
    "model_dp = nn.DataParallel(model)\n",
    "\n",
    "# Save checkpoint in expected format\n",
    "gradcam_checkpoint = {\n",
    "    'epoch': checkpoint['epoch'],\n",
    "    'state_dict': model_dp.state_dict(),  # DataParallel state dict\n",
    "    'best_f1': best_f1,\n",
    "    'metrics': val_metrics,\n",
    "    'crop_size': CROP_SIZE,\n",
    "    'approach': 'localized',\n",
    "    'note': 'This model expects cropped 128x128 input centered on target vertebra'\n",
    "}\n",
    "\n",
    "save_path = os.path.join(CHECKPOINT_FOLDER, 'seresnet50_classifier_localized.tar')\n",
    "torch.save(gradcam_checkpoint, save_path)\n",
    "print(f\"\\nGrad-CAM compatible checkpoint saved to:\")\n",
    "print(f\"  {save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTANT: GRAD-CAM USAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"This model expects {CROP_SIZE}x{CROP_SIZE} cropped input!\")\n",
    "print(f\"You need to modify grad_CAM_3d_sagittal.py to:\")\n",
    "print(f\"  1. Load mask_2d for each CT file\")\n",
    "print(f\"  2. Find bounding box of target vertebra\")\n",
    "print(f\"  3. Crop CT to {CROP_SIZE}x{CROP_SIZE} around center\")\n",
    "print(f\"  4. Generate heatmap on cropped region\")\n",
    "print(f\"  5. Map heatmap back to original coordinates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b36ad7e",
   "metadata": {},
   "source": [
    "## 14. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07100038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Healthy', 'Fractured'],\n",
    "            yticklabels=['Healthy', 'Fractured'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix (WITH Localization)\\nAccuracy: {val_metrics[\"accuracy\"]:.2%}, F1: {val_metrics[\"f1\"]:.4f}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_FOLDER, 'confusion_matrix_localized.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed049185",
   "metadata": {},
   "source": [
    "## Summary: Approach Comparison\n",
    "\n",
    "### This Notebook (WITH Localization)\n",
    "- **Input**: Cropped 128×128 region around target vertebra (using mask_2d)\n",
    "- **Pros**: Model sees ONLY target vertebra, cleaner learning signal\n",
    "- **Cons**: Requires mask at inference, need to modify Grad-CAM script\n",
    "\n",
    "### Other Notebook (WITHOUT Localization)\n",
    "- **Input**: Full 256×256 slice (target centered, neighbors visible)\n",
    "- **Pros**: Simpler pipeline, works with existing Grad-CAM script\n",
    "- **Cons**: Model might learn features from neighboring vertebrae\n",
    "\n",
    "### Compare Results\n",
    "Run both notebooks and compare:\n",
    "1. Validation F1 scores\n",
    "2. Confusion matrices\n",
    "3. Grad-CAM heatmaps (where does the model focus?)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
